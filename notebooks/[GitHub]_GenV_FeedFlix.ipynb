{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJCfDyyKRv3P"
      },
      "source": [
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AMunkq7Mdb"
      },
      "source": [
        "# FeedFlix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbt69GlmAHU4"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MyZdmMvlAOfe"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies\n",
        "!pip install --upgrade --quiet google-genai\n",
        "!pip install --quiet mediapy\n",
        "!pip install --quiet moviepy==2.1.2\n",
        "!pip install --quiet 'git+https://github.com/google-marketing-solutions/gen-v.git@main#egg=gtech-gen-v&subdirectory=backend'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScOkw_T7Adfl"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "import base64\n",
        "import concurrent.futures\n",
        "import contextlib\n",
        "import google.auth\n",
        "import ipywidgets as widgets\n",
        "import mediapy as media\n",
        "import moviepy as mp\n",
        "import os\n",
        "import pydantic\n",
        "import requests\n",
        "import sys\n",
        "import time\n",
        "\n",
        "from datetime import datetime, date\n",
        "from IPython.display import clear_output, Markdown\n",
        "from gen_v import models\n",
        "from gen_v import storage as gcs\n",
        "from gen_v import utils\n",
        "from gen_v import video\n",
        "from google import genai\n",
        "from google.cloud import storage\n",
        "from google.genai.types import (\n",
        "    GenerateContentConfig,\n",
        "    MediaResolution,\n",
        "    Part\n",
        ")\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageDraw, ImageFont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC61ooM4Ee8T"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate User\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6HWaRHf9QZ-"
      },
      "source": [
        "\n",
        "## Parameters Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjLklHMq7hZC"
      },
      "outputs": [],
      "source": [
        "# @title GCP Parameters\n",
        "\n",
        "GCP_PROJECT_ID = 'your-project-id'  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "GEMINI_PROJECT_ID = GCP_PROJECT_ID\n",
        "IMAGEN_PROJECT_ID = GCP_PROJECT_ID\n",
        "VEO_PROJECT_ID = GCP_PROJECT_ID\n",
        "\n",
        "GCP_BUCKET_NAME = 'your-bucket-name' # @param {type: \"string\", placeholder: \"bucket-name-without-path\"}\n",
        "INPUT_IMAGE_BUCKET_NAME = GCP_BUCKET_NAME\n",
        "FOLDER_NAME = 'your-folder-name' #@param {type: \"string\"}\n",
        "INPUT_IMAGE_BUCKET_PATH = f'{FOLDER_NAME}/input-images/'\n",
        "INPUT_VIDEOS_BUCKET_PATH = f'{FOLDER_NAME}/input-videos/'\n",
        "INPUT_AUDIO_BUCKET_PATH = f'{FOLDER_NAME}/audio/'\n",
        "\n",
        "OUTPUT_IMAGES_BUCKET_NAME = GCP_BUCKET_NAME\n",
        "OUTPUT_IMAGES_BUCKET_PATH = f'{FOLDER_NAME}/output-images/'\n",
        "OUTPUT_VIDEOS_BUCKET_NAME = GCP_BUCKET_NAME\n",
        "OUTPUT_VIDEOS_BUCKET_PATH = f'{FOLDER_NAME}/output-videos/'\n",
        "OUTPUT_VIDEOS_URI = f\"gs://{OUTPUT_VIDEOS_BUCKET_NAME}/{OUTPUT_VIDEOS_BUCKET_PATH}\"\n",
        "\n",
        "\n",
        "TMP_STRING = '/content'\n",
        "\n",
        "# Set current date variables, used in GCS URI paths\n",
        "CURRENT_YEAR, CURRENT_WEEK, _ = date.today().isocalendar()\n",
        "WEEK_AND_YEAR = f\"week{CURRENT_WEEK}-{CURRENT_YEAR}\"\n",
        "\n",
        "IMAGE_OVERLAYS_PATH = f\"{OUTPUT_VIDEOS_BUCKET_NAME}/{OUTPUT_VIDEOS_BUCKET_PATH}image_overlays/{WEEK_AND_YEAR}\"\n",
        "FINAL_OVERLAYS_PATH = f\"{OUTPUT_VIDEOS_BUCKET_NAME}/{OUTPUT_VIDEOS_BUCKET_PATH}final_overlays/{WEEK_AND_YEAR}\"\n",
        "IMAGES_URI = f\"{INPUT_IMAGE_BUCKET_NAME}/{INPUT_IMAGE_BUCKET_PATH}{WEEK_AND_YEAR}\"\n",
        "OUTPUT_URI_PATH = f\"{OUTPUT_IMAGES_BUCKET_NAME}/{OUTPUT_IMAGES_BUCKET_PATH}{WEEK_AND_YEAR}\"\n",
        "\n",
        "VEO_OVERLAYS_FOLDER = f'{OUTPUT_VIDEOS_BUCKET_NAME}/{OUTPUT_VIDEOS_BUCKET_PATH}final_overlays/'\n",
        "VEO_CLIPS_URI = f\"{VEO_OVERLAYS_FOLDER}{WEEK_AND_YEAR}/\"\n",
        "\n",
        "INTROS_OUTROS_URI = f'{OUTPUT_VIDEOS_BUCKET_NAME}/{INPUT_VIDEOS_BUCKET_PATH}'\n",
        "AUDIO_URI = f'{OUTPUT_VIDEOS_BUCKET_NAME}/{INPUT_AUDIO_BUCKET_PATH}'\n",
        "\n",
        "STITCHING_OUTPUT_URI = f'{OUTPUT_VIDEOS_URI}concatenated/{WEEK_AND_YEAR}/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfksQatIAg5q"
      },
      "outputs": [],
      "source": [
        "#@title GenAI models parameters\n",
        "if not VEO_PROJECT_ID or VEO_PROJECT_ID == \"[your-project-id]\":\n",
        "    VEO_PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "if not VEO_PROJECT_ID or VEO_PROJECT_ID == \"[your-project-id]\":\n",
        "    VEO_PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")\n",
        "\n",
        "imagen_client = genai.Client(vertexai=True, project=IMAGEN_PROJECT_ID, location=LOCATION)\n",
        "veo_client = genai.Client(vertexai=True, project=VEO_PROJECT_ID, location=LOCATION) # Not used since using REST\n",
        "\n",
        "# Set VEO Model versions\n",
        "video_model = \"veo-2.0-generate-001\"\n",
        "gemini_model = \"gemini-2.0-flash\"\n",
        "\n",
        "# Setup REST API video model and endpoints\n",
        "video_model_uri = f\"https://us-central1-aiplatform.googleapis.com/v1beta1/projects/{VEO_PROJECT_ID}/locations/us-central1/publishers/google/models/{video_model}\"\n",
        "prediction_endpoint = f\"{video_model_uri}:predictLongRunning\"\n",
        "fetch_endpoint = f\"{video_model_uri}:fetchPredictOperation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57MS2BdC77bh"
      },
      "outputs": [],
      "source": [
        "# @title Image Editing Parameters\n",
        "\n",
        "RESIZED_IMAGE_WIDTH = 1280 # @param\n",
        "RESIZED_IMAGE_HEIGHT = 720 # @param\n",
        "# @markdown ----\n",
        "\n",
        "# @markdown Enable this to show images during processing\n",
        "COLOR_BACKGROUND_REPLACEMENT = True # @param {type: 'boolean'}\n",
        "\n",
        "BACKGROUND_RED = 255 # @param\n",
        "BACKGROUND_GREEN = 224 # @param\n",
        "BACKGROUND_BLUE = 77 # @param\n",
        "BACKGROUND_TRANSPARENCY = 255\n",
        "\n",
        "ORIGINAL_BACKGROUND_COLOR = models.RGBColor.from_tuple((255,255,255))\n",
        "BACKGROUND_COLOR = models.RGBColor.from_tuple((\n",
        "    BACKGROUND_RED,\n",
        "    BACKGROUND_GREEN,\n",
        "    BACKGROUND_BLUE\n",
        "))\n",
        "\n",
        "\n",
        "# Use this to convert HEX color to RGB\n",
        "# hex_color = \"#fcdc4c\"\n",
        "# rgb_color = utils.hex_to_rgb(hex_color)\n",
        "# print(rgb_color)  # Output: (R, G, B)\n",
        "\n",
        "# @markdown ----\n",
        "\n",
        "# @markdown Enable this to show images during processing\n",
        "ENABLE_SHOW_IMAGES_INLINE = False # @param {type: \"boolean\"}\n",
        "SHOW_IMAGE_HEIGHT = 500 # @param [250,500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0423lHbe9BzW"
      },
      "outputs": [],
      "source": [
        "# @title Video Generation Parameters\n",
        "\n",
        "# @markdown Set VEO parameters\n",
        "DURATION = 5  # @param {type:\"slider\", min:5, max:8, step:1}\n",
        "SAMPLE_COUNT = 2  # @param {type:\"slider\", min:1, max:4, step:1}\n",
        "NEGATIVE_PROMPT = \"copyrighted content\"  # @param {type: 'string'}\n",
        "PROMPT_ENHANCE = True  # @param {type: 'boolean'}\n",
        "PERSON_GENERATION = \"allow_adult\"  #@param [\"allow_adult\", \"dont_allow\"]\n",
        "\n",
        "# @markdown ----\n",
        "\n",
        "# @markdown Decide which prompt to use\n",
        "PROMPT_TYPE = \"CUSTOM\" # @param [\"CUSTOM\", \"GEMINI\"]\n",
        "# @markdown Your Veo prompt aka CUSTOM\n",
        "CUSTOM_VIDEO_PROMPT = \"Animate this image in a way that is most appropriate for the content in the image\" # @param [\"Animate this image in a way that is most appropriate for the content in the image\"] {\"allow-input\":true}\n",
        "# @markdown Let Gemini generate a video prompt aka GEMINI\n",
        "GENERATE_VIDEO_PROMPT = \"Analyse the image and write a prompt for a generative video AI to animate the video in the most appropriate way for the content to be displayed in an online ad.  Consider the function of the main object in the image when deciding how to animate it.  If there is a background, focus on animating the primary object only. Output the prompt only. Don't show any of the anlaysis or headings in your response, only provide the prompt you created.\" # @param [\"Analyse the image and write a prompt for a generative video AI to animate the video in the most appropriate way for the content to be displayed in an online ad.  Consider the function of the main object in the image when deciding how to animate it.  If there is a background, focus on animating the primary object only. Output the prompt only. Don't show any of the anlaysis or headings in your response, only provide the prompt you created.\"] {\"allow-input\":true}\n",
        "\n",
        "# @markdown ----\n",
        "\n",
        "VIDEO_ORIENTATION = \"LANDSCAPE\" # @param [\"LANDSCAPE\", \"PORTRAIT\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caeI68sF9IkM"
      },
      "outputs": [],
      "source": [
        "# @title Overlay Parameters\n",
        "\n",
        "LOGO_FILE_NAME = \"logo.png\" #@param {type: 'string'}\n",
        "LOGO_URI = f'gs://{GCP_BUCKET_NAME}/{FOLDER_NAME}/logos/{LOGO_FILE_NAME}'\n",
        "STICKER_FILE_NAME = \"sticker.png\" #@param {type: 'string'}\n",
        "STICKER_URI = f'gs://{GCP_BUCKET_NAME}/{FOLDER_NAME}/input-overlays/{STICKER_FILE_NAME}'\n",
        "FONT_FILE_NAME = \"font.ttf\" #@param {type: 'string'}\n",
        "FONT_URI = f'gs://{GCP_BUCKET_NAME}/{FOLDER_NAME}/fonts/{FONT_FILE_NAME}'\n",
        "\n",
        "OVERLAY_WIDTH = RESIZED_IMAGE_WIDTH\n",
        "OVERLAY_HEIGTH = RESIZED_IMAGE_HEIGHT\n",
        "\n",
        "LOGO_POSITION = (50, 520) #@param\n",
        "LOGO_DESIRED_HEIGHT = 150 #@param Set as 0 to avoid auto-scaling\n",
        "LOGO_START = 0 #@param\n",
        "LOGO_DURATION = 5 #@param\n",
        "\n",
        "\n",
        "STICKER_POSITION = (50, 50) #@param\n",
        "STICKER_START = 0 #@param\n",
        "STICKER_DURATION = 5 #@param\n",
        "STICKER_DESIRED_HEIGHT = 100 #@param\n",
        "\n",
        "TEXT_FONT_SIZE = 30 #@param\n",
        "TEXT_START = 0 #@param\n",
        "TEXT_DURATION = 5 #@param\n",
        "TEXT_COLOR = 'blue'   #@param\n",
        "TEXT_POSITION = (850, 50)  #@param\n",
        "\n",
        "GCS_IMAGES_TEST: list[models.ImageInput] = [\n",
        "    models.ImageInput(\n",
        "        path=LOGO_URI,\n",
        "        start=LOGO_START,\n",
        "        position=LOGO_POSITION,\n",
        "        duration=LOGO_DURATION,\n",
        "        height=LOGO_DESIRED_HEIGHT\n",
        "      ),\n",
        "    models.ImageInput(\n",
        "        path=STICKER_URI,\n",
        "        start=STICKER_START,\n",
        "        position=STICKER_POSITION,\n",
        "        duration=STICKER_DURATION,\n",
        "        height=STICKER_DESIRED_HEIGHT\n",
        "      )\n",
        "    ]\n",
        "\n",
        "TEXT_TEST = models.TextInput(\n",
        "    text=\"text_to_display\",\n",
        "    font=FONT_URI,\n",
        "    font_size=TEXT_FONT_SIZE,\n",
        "    start_time=TEXT_START,\n",
        "    duration=TEXT_DURATION,\n",
        "    color=TEXT_COLOR,\n",
        "    position=TEXT_POSITION\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcDkmy2J_GIO"
      },
      "outputs": [],
      "source": [
        "# @title Stitching Parameters\n",
        "OUTPUT_WITHOUT_AUDIO = 'video_transition_demo.mp4'\n",
        "OUTPUT_WITH_AUDIO = 'video_transition_demo_audio.mp4'\n",
        "\n",
        "VERBOSE = False # @param {type:\"boolean\"}\n",
        "TRANSITION = \"SWIPE\" # @param [\"CROSS_FADE\", \"FADE_IN\", \"SWIPE\", \"SLIDE_IN\"]\n",
        "TRANSITON_DURATION = 0.5 # @param {type:\"number\"}\n",
        "TRANSITION_SIDE = \"left\" # @param [\"left\", \"right\", \"top\", \"bottom\"]\n",
        "\n",
        "TRANSITION_TEST = models.VideoTransition(\n",
        "    name=TRANSITION,\n",
        "    padding=TRANSITON_DURATION,\n",
        "    side=TRANSITION_SIDE\n",
        ")\n",
        "OUTPUT_LENGHT = 23 # @param {type:\"slider\", min:10, max:30, step:1}\n",
        "TRIM_FROM = \"end\" # @param [\"start\", \"end\"]\n",
        "TRIM_ENABLED = True # @param {type:\"boolean\"}\n",
        "\n",
        "UPSCALE_FACTOR = 2 #@param"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wifFZ-HbB6"
      },
      "source": [
        "## FeedFlix Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzich0jIj1bU"
      },
      "outputs": [],
      "source": [
        "# @title Video Generation Functions\n",
        "\n",
        "def fetch_operation(lro_name: str) -> str | None:\n",
        "    \"\"\"Fetches the status of a long-running operation.\n",
        "\n",
        "    Args:\n",
        "        lro_name: The name of the long-running operation.\n",
        "\n",
        "    Returns:\n",
        "        The response from the API containing the operation status.\n",
        "\n",
        "    \"\"\"\n",
        "    request = {\"operationName\": lro_name}\n",
        "    # The generation usually takes 2 minutes. Loop 30 times, around 5 minutes.\n",
        "    max_attempts = 30  # Maximum number of attempts\n",
        "    attempt_interval = 10  # Seconds between attempts\n",
        "\n",
        "    for i in range(max_attempts):\n",
        "      try:\n",
        "        resp = video.send_request_to_google_api(fetch_endpoint, request)\n",
        "        if \"done\" in resp and resp[\"done\"]:\n",
        "            return resp\n",
        "      except Exception as e:\n",
        "            print(f\"Error raised while fetching operation: {e}\")\n",
        "\n",
        "      time.sleep(attempt_interval)\n",
        "\n",
        "def image_to_video(\n",
        "    prompt: str,\n",
        "    image_uri: str,\n",
        "    gcs_uri: str,\n",
        "    duration: int,\n",
        "    sample_count: int,\n",
        "    aspect_ratio: str,\n",
        "    negative_prompt: str = \"\",\n",
        "    prompt_enhance: bool = True,\n",
        "    person_generation: str = \"allow_adult\",\n",
        ") -> str | None:\n",
        "    \"\"\"Generates a video from an image using the Video Generation API.\n",
        "\n",
        "    Args:\n",
        "        prompt: The text prompt for video generation.\n",
        "        image_uri: The GCS URI of the input image.\n",
        "        gcs_uri: The GCS URI where the generated video will be stored.\n",
        "        duration: The desired duration of the video in seconds.\n",
        "        sample_count: The number of video samples to generate.\n",
        "        aspect_ratio: The aspect ratio of the video (e.g., \"16:9\").\n",
        "        negative_prompt: Text describing what shouldn't be included in the video.\n",
        "        prompt_enhance: Whether to enhance the prompt (default: True).\n",
        "        person_generation: Settings for person generation in the video.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the response from the Video Generation API,\n",
        "        including the operation details and generated video information.\n",
        "\n",
        "    \"\"\"\n",
        "    request_model = models.VeoApiRequest(\n",
        "        prompt=prompt,\n",
        "        image_uri=image_uri,\n",
        "        gcs_uri=gcs_uri,\n",
        "        duration=duration,\n",
        "        sample_count=sample_count,\n",
        "        aspect_ratio=aspect_ratio,\n",
        "        negative_prompt=negative_prompt,\n",
        "        prompt_enhance=prompt_enhance,\n",
        "        person_generation=person_generation\n",
        "    )\n",
        "    req = request_model.to_api_payload()\n",
        "    try:\n",
        "      resp = video.send_request_to_google_api(prediction_endpoint, req)\n",
        "      # print(f\"Video generation request submitted: {resp}\")\n",
        "      return fetch_operation(resp[\"name\"])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending image_to_video request: {e}\")\n",
        "\n",
        "def generate_videos_and_download(\n",
        "    prompt: str,\n",
        "    input_image_uri: str,  # GCS URI of the input image\n",
        "    input_image_path: str,  # Local path of the input image\n",
        "    aspect_ratio: str,\n",
        "    output_file_prefix: str,\n",
        "    product: dict[str, any]\n",
        ") -> list[dict[str, any]]:\n",
        "    \"\"\"Generates videos, downloads them, and returns their information.\n",
        "\n",
        "    Args:\n",
        "        prompt: The text prompt for video generation.\n",
        "        input_image_uri: The GCS URI of the input image.\n",
        "        input_image_path: The local path of the input image.\n",
        "        aspect_ratio: The aspect ratio of the video (e.g., \"16:9\").\n",
        "        output_file_prefix: The prefix for output video file names.\n",
        "        product: A dictionary containing product information.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, containing information about a generated video\n",
        "    \"\"\"\n",
        "    try:\n",
        "      output_videos = image_to_video(prompt,\n",
        "                                    input_image_uri,\n",
        "                                    f\"{OUTPUT_VIDEOS_URI}veo/{WEEK_AND_YEAR}\",\n",
        "                                    DURATION,\n",
        "                                    SAMPLE_COUNT,\n",
        "                                    aspect_ratio,\n",
        "                                    NEGATIVE_PROMPT,\n",
        "                                    PROMPT_ENHANCE,\n",
        "                                    PERSON_GENERATION)\n",
        "      file_name = gcs.get_file_name_from_gcs_url(input_image_uri)\n",
        "      output_video_files = [] # Array to hold files generated\n",
        "      print(\n",
        "          f'Generated videos {len(output_videos[\"response\"][\"videos\"])}'\n",
        "          f' for {file_name}'\n",
        "      )\n",
        "\n",
        "      for video in output_videos[\"response\"][\"videos\"]:\n",
        "          veo_name = gcs.get_file_name_from_gcs_url(video[\"gcsUri\"])\n",
        "          output_video_local_path = (\n",
        "              f\"{file_name}-{output_file_prefix}-{veo_name}\"\n",
        "          )\n",
        "\n",
        "          gcs.download_file_locally(video[\"gcsUri\"], output_video_local_path)\n",
        "\n",
        "          output_video_files.append({\n",
        "              \"gcs_uri\": video[\"gcsUri\"],\n",
        "              \"local_file\": output_video_local_path,\n",
        "              \"local_file_name\": output_video_local_path.split(\"/\")[-1],\n",
        "              \"product_title\": product['title'],\n",
        "              \"promo_text\": \"\"\n",
        "          })\n",
        "\n",
        "      return output_video_files\n",
        "\n",
        "    except Exception as e:\n",
        "      print(f\"Error in generate_videos_and_download: {e}\")\n",
        "\n",
        "def generate_videos_concurrently(selected_products: list[dict]) -> list[dict]:\n",
        "    \"\"\"Generates videos concurrently for multiple products and returns info.\n",
        "\n",
        "    Args:\n",
        "        selected_products: A list of dictionaries, with products\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries of generated videos.\n",
        "    \"\"\"\n",
        "\n",
        "    output_video_files = [] # Array to hold the output video files\n",
        "\n",
        "    def generate_video(product: dict) -> list[dict]:\n",
        "        \"\"\"Generates a video for a given product and returns video information.\n",
        "\n",
        "        Args:\n",
        "            product: A dictionary containing product information\n",
        "\n",
        "        Returns:\n",
        "            A list of dictionaries with generated videos info:\n",
        "        \"\"\"\n",
        "        try:\n",
        "          recolored_image_uri = product['recolored_image_uri']\n",
        "          recolored_image_local_path = gcs.download_file_locally(\n",
        "              recolored_image_uri\n",
        "          )\n",
        "          prompt = \"\"\n",
        "\n",
        "          # Determine the prompt based on PROMPT_TYPE\n",
        "          if PROMPT_TYPE == \"CUSTOM\":\n",
        "              prompt = CUSTOM_VIDEO_PROMPT\n",
        "          elif PROMPT_TYPE == \"GEMINI\":\n",
        "            gemini_prompt_request = models.GeminiPromptRequest(\n",
        "                prompt_text=GENERATE_VIDEO_PROMPT,\n",
        "                image_file_path=recolored_image_local_path,\n",
        "                model_name=gemini_model\n",
        "            )\n",
        "            prompt = video.get_gemini_generated_video_prompt(\n",
        "                gemini_prompt_request,\n",
        "                project_id=GEMINI_PROJECT_ID,\n",
        "                location=LOCATION\n",
        "            )\n",
        "          else:\n",
        "              # Default to custom prompt if invalid PROMPT_TYPE\n",
        "              prompt = CUSTOM_VIDEO_PROMPT\n",
        "              print(\n",
        "                  f\"Invalid PROMPT_TYPE: {PROMPT_TYPE}. \"\n",
        "                  f\"Using CUSTOM_VIDEO_PROMPT.\"\n",
        "              )\n",
        "\n",
        "          if VIDEO_ORIENTATION == \"LANDSCAPE\":\n",
        "              aspect_ratio = \"16:9\"\n",
        "              output_file_prefix = \"video-landscape\"\n",
        "          elif VIDEO_ORIENTATION == \"PORTRAIT\":\n",
        "              aspect_ratio = \"9:16\"\n",
        "              output_file_prefix = \"video-portrait\"\n",
        "          else:\n",
        "              aspect_ratio = \"16:9\"  # Default\n",
        "              output_file_prefix = \"video-landscape\"\n",
        "              print(\n",
        "                  f\"Invalid VIDEO_ORIENTATION: {VIDEO_ORIENTATION}. \"\n",
        "                  f\"Using LANDSCAPE.\"\n",
        "              )\n",
        "\n",
        "          # Generate landscape videos using selected prompt\n",
        "          output_video_files.extend(\n",
        "              generate_videos_and_download(\n",
        "                prompt,\n",
        "                recolored_image_uri,\n",
        "                recolored_image_local_path,\n",
        "                aspect_ratio,\n",
        "                output_file_prefix,\n",
        "                product))\n",
        "        except Exception as e:\n",
        "          print(f'Error generating video for product {e}')\n",
        "\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        executor.map(generate_video, selected_products)\n",
        "\n",
        "    return output_video_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtEzvgCPI8MH"
      },
      "outputs": [],
      "source": [
        "# @title Get User Video Selection Functions\n",
        "def get_user_choice_with_videos(videos: list[dict]) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Allows the user to choose multiple options from a list with video thumbnails.\n",
        "\n",
        "    Args:\n",
        "        videos: A list of dictionaries, where each dictionary represents a video\n",
        "                and contains keys \"local_file\" for the local video path,\n",
        "                \"product_title\" for the video title, and \"promo_text\"\n",
        "                for any promotional text.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, each containing gcs_url, and title\n",
        "        of the selected videos.\n",
        "    \"\"\"\n",
        "    selected_videos = []\n",
        "    # Create checkboxes and video widgets\n",
        "    checkboxes = [widgets.Checkbox(\n",
        "        value=False,\n",
        "        description=f\"{video['product_title']}\") for video in videos]\n",
        "\n",
        "    # Create video elements using local file paths\n",
        "    video_elements = []\n",
        "    for video in videos:\n",
        "        local_video_path = video['local_file']\n",
        "\n",
        "        # Encode the video file as base64\n",
        "        with open(local_video_path, \"rb\") as f:\n",
        "            video_data = f.read()\n",
        "        encoded_video = base64.b64encode(video_data).decode()\n",
        "\n",
        "        # Create video element using base64 encoded data\n",
        "        video_element = f\"\"\"\n",
        "        <video width=\"320\" height=\"240\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{encoded_video}\" type=\"video/mp4\">\n",
        "            Your browser does not support the video tag.\n",
        "        </video>\n",
        "        \"\"\"\n",
        "        video_elements.append(widgets.HTML(value=video_element))\n",
        "\n",
        "    # Create a text field for promo_title\n",
        "    promo_title_inputs = [widgets.Text(\n",
        "        value=\"\",\n",
        "        placeholder=\"Enter promo text:\",\n",
        "        description=f\"{video['promo_text']}\") for video in videos]\n",
        "\n",
        "    # Create a container to arrange checkboxes and videos\n",
        "    items = []\n",
        "    for checkbox, video_widget, promot_title_input in zip(\n",
        "        checkboxes, video_elements, promo_title_inputs\n",
        "    ):\n",
        "      items.extend([\n",
        "          video_widget,\n",
        "          checkbox,\n",
        "          widgets.HTML(value=\"<br>\"),\n",
        "          promot_title_input\n",
        "      ])\n",
        "    container = widgets.VBox(items)\n",
        "\n",
        "    display(container)\n",
        "\n",
        "    def on_button_clicked(button):\n",
        "        \"\"\"This function processes the selected videos and promo texts, and\n",
        "        provides feedback to the user.\n",
        "        \"\"\"\n",
        "        try:\n",
        "          global selected_videos\n",
        "          clear_output(wait=True)  # Clear previous output\n",
        "          for i, checkbox in enumerate(checkboxes):\n",
        "            if checkbox.value:\n",
        "                video = videos[i]\n",
        "                selected_videos.append(video)\n",
        "                video['promo_text'] = promo_title_inputs[i].value\n",
        "          print(selected_videos)\n",
        "          return selected_videos\n",
        "        except Exception as e:\n",
        "          print(f'Error on_button_click {e} in get_user_choice_with_videos()')\n",
        "\n",
        "    submit_button = widgets.Button(description=\"Submit\")\n",
        "    submit_button.on_click(on_button_clicked)\n",
        "    display(submit_button)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miw6m9kllX1t"
      },
      "outputs": [],
      "source": [
        "#@title Overlay Functions\n",
        "\n",
        "def add_text_clips_to_video(\n",
        "    input_video: models.VideoInput,\n",
        "    text_inputs: list[models.TextInput],\n",
        "    output_video_path: models.VideoInput\n",
        ") -> None:\n",
        "  \"\"\"Adds text to a video clip.\n",
        "  Args:\n",
        "    video_path: Path to the video file.\n",
        "    text_inputs: List of TextInput objects, each representing the text to add.\n",
        "    output_path: Path to save the output video.\n",
        "  \"\"\"\n",
        "  local_video_path = gcs.download_file_locally(input_video.path)\n",
        "  input_file_name = gcs.get_file_name_from_gcs_url(input_video.path)\n",
        "  local_output_video_path = f\"{TMP_STRING}/output_{input_file_name}\"\n",
        "\n",
        "  for text in text_inputs:\n",
        "    local_font_path = gcs.download_file_locally(text.font)\n",
        "    text.font = local_font_path\n",
        "\n",
        "  with video.load_text_clips(local_video_path, text_inputs) as clips:\n",
        "    with mp.CompositeVideoClip(clips) as final_clip:\n",
        "      final_clip.write_videofile(local_output_video_path, codec='libx264')\n",
        "      output_uri = output_video_path.path\n",
        "      gcs.upload_file_to_gcs(local_output_video_path, output_uri)\n",
        "      final_clip.close()\n",
        "\n",
        "\n",
        "def process_videos_with_overlays_and_text(\n",
        "    videos: list[dict],\n",
        "    images: list[models.ImageInput],\n",
        "    overlay_text: models.TextInput,\n",
        "    overlays_uri: str,\n",
        "    final_uri: str\n",
        ") -> None:\n",
        "    \"\"\"Processes videos by adding image and text overlays and uploading to gcs.\n",
        "\n",
        "    Args:\n",
        "        videos: A list of video dictionaries with GCS URI and local file path.\n",
        "        images: A list of `ImageInput` image overlays to be added to the videos.\n",
        "        overlay_text: A `TextInput` object, defining the text overlay.\n",
        "        overlays_uri: The GCS URI where intermediate overlays will be stored.\n",
        "        final_uri: The GCS URI where final videos with overlays will be stored.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    print(f'process_videos_with_overlays_and_text...')\n",
        "\n",
        "    def process_video(video: dict):\n",
        "      \"\"\"Processes a single video by adding overlays and text.\n",
        "\n",
        "      Args:\n",
        "        video: A dictionary representing a video\n",
        "      \"\"\"\n",
        "      print(f'process_video: {video}')\n",
        "      local_video_file_path = gcs.download_file_locally(\n",
        "          video['gcs_uri'], video['local_file_name'])\n",
        "      local_video_file = models.VideoInput(path = local_video_file_path)\n",
        "\n",
        "      gcs_file_name = video['local_file_name']\n",
        "      gcs_image_overlay_video_path = f\"{overlays_uri}/{gcs_file_name}\"\n",
        "      image_overlay_video = models.VideoInput(\n",
        "          path = f'gs://{gcs_image_overlay_video_path}')\n",
        "\n",
        "      video.overlay_image_on_video(local_video_file, images, image_overlay_video)\n",
        "\n",
        "      promo_text = models.TextInput(\n",
        "          text=video['promo_text'],\n",
        "          font=overlay_text.font,\n",
        "          font_size=overlay_text.font_size,\n",
        "          start_time=overlay_text.start_time,\n",
        "          duration=overlay_text.duration,\n",
        "          color=overlay_text.color,\n",
        "          position=overlay_text.position\n",
        "      )\n",
        "\n",
        "      # Define the GCS path for the final video with text overlay.\n",
        "      final_video_gcs_path = f\"{final_uri}/{gcs_file_name}\"\n",
        "      final_video = models.VideoInput(path = final_video_gcs_path)\n",
        "\n",
        "      add_text_clips_to_video(\n",
        "        image_overlay_video,\n",
        "        [promo_text],\n",
        "        final_video\n",
        "      )\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "      executor.map(process_video, videos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_GYY69Iu3uG"
      },
      "outputs": [],
      "source": [
        "### (Optional) Visual Overlay Layout\n",
        "def draw_elements_on_image(\n",
        "    image_path: str,\n",
        "    logo_path: str,\n",
        "    sticker_path: str,\n",
        "    font_path: str\n",
        ") -> PIL_Image:\n",
        "    \"\"\"Draws elements on an image based on the given parameters.\n",
        "\n",
        "    Args:\n",
        "        image_path: The path to the base image file.\n",
        "        logo_path: The path to the logo image file.\n",
        "        sticker_path: The path to the sticker image file.\n",
        "        font_path: The path to the font file.\n",
        "\n",
        "    Returns:\n",
        "        A PIL Image object with the elements drawn on it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      img = PIL_Image.new('RGB', (OVERLAY_WIDTH, OVERLAY_HEIGTH), color='yellow')\n",
        "      draw = ImageDraw.Draw(img)\n",
        "\n",
        "      draw_logo(draw, logo_path)\n",
        "      draw_sticker(draw, sticker_path)\n",
        "      draw_text(draw, font_path)\n",
        "\n",
        "      img.save(image_path)\n",
        "    except (FileNotFoundError, PIL_Image.UnidentifiedImageError, OSError) as e:\n",
        "      print(f\"Error drawing elements on image: {e}\")\n",
        "\n",
        "def draw_logo(draw: ImageDraw, logo_path:str) -> None:\n",
        "  \"\"\"Draws a logo on the image.\n",
        "\n",
        "    Args:\n",
        "        draw: The ImageDraw object to draw on.\n",
        "        logo_path: The path to the logo image file.\n",
        "  \"\"\"\n",
        "  logo = PIL_Image.open(logo_path)\n",
        "  logo_width, logo_height = logo.size\n",
        "  if LOGO_DESIRED_HEIGHT > 0:\n",
        "    logo_width = int(logo_width * LOGO_DESIRED_HEIGHT / logo_height)\n",
        "    logo_height = LOGO_DESIRED_HEIGHT\n",
        "    logo = logo.resize((logo_width, logo_height))\n",
        "  draw.rectangle(\n",
        "    [LOGO_POSITION, (LOGO_POSITION[0] + logo_width, LOGO_POSITION[1] + logo_height)],\n",
        "    outline=\"red\"\n",
        "  )\n",
        "\n",
        "def draw_sticker(draw: ImageDraw, sticker_path: str) -> None:\n",
        "  \"\"\"Draws a sticker on the image.\n",
        "\n",
        "  Args:\n",
        "      draw: The ImageDraw object to draw on.\n",
        "      sticker_path: The path to the sticker image file.\n",
        "  \"\"\"\n",
        "  sticker = PIL_Image.open(sticker_path)\n",
        "  sticker_width, sticker_height = sticker.size\n",
        "  if STICKER_DESIRED_HEIGHT > 0:\n",
        "    sticker_width = int(sticker_width * STICKER_DESIRED_HEIGHT / sticker_height)\n",
        "    sticker_height = STICKER_DESIRED_HEIGHT\n",
        "    sticker = sticker.resize((sticker_width, sticker_height))\n",
        "  draw.rectangle([\n",
        "      STICKER_POSITION,\n",
        "     (STICKER_POSITION[0] + sticker_width, STICKER_POSITION[1] + sticker_height)],\n",
        "    outline=\"green\"\n",
        "  )\n",
        "\n",
        "def draw_text(draw: ImageDraw, font_path: str) -> None:\n",
        "  \"\"\"Draws text on the image.\n",
        "\n",
        "    Args:\n",
        "        draw: The ImageDraw object to draw on.\n",
        "        font_path: The path to the font file.\n",
        "  \"\"\"\n",
        "  font = ImageFont.truetype(font_path, TEXT_FONT_SIZE)\n",
        "  draw.text(TEXT_POSITION, \"Sample Text\", font=font, fill=TEXT_COLOR)\n",
        "\n",
        "\n",
        "def overlay_elements_on_image_and_display(image_path: str) -> None:\n",
        "    \"\"\"Overlays elements (logo, sticker, text) on an sample image and displays\n",
        "    the image to help visualize the overlays layout.\n",
        "\n",
        "    Args:\n",
        "        image_path: The path to the image file on which to overlay the elements.\n",
        "\n",
        "    \"\"\"\n",
        "    # Download element files from GCS if needed\n",
        "    logo_path = gcs.download_file_locally(LOGO_URI)\n",
        "    sticker_path = gcs.download_file_locally(STICKER_URI)\n",
        "    font_path = gcs.download_file_locally(FONT_URI)\n",
        "\n",
        "    # Draw elements on the image\n",
        "    draw_elements_on_image(image_path, logo_path, sticker_path, font_path)\n",
        "\n",
        "    # Display the modified image\n",
        "    video.display_image(image_path, height=SHOW_IMAGE_HEIGHT)\n",
        "\n",
        "overlay_elements_on_image_and_display(\"output_image.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxdUjPntA_Jw"
      },
      "outputs": [],
      "source": [
        "#@title Video Stitching Functions\n",
        "def select_videos_for_concatenation(\n",
        "    local_video_paths: list[str],\n",
        "    transition: models.VideoTransition,\n",
        "    output_length: int,\n",
        "    trim_location: str,\n",
        "    audio_inputs: list[models.AudioInput],\n",
        "    gcs_uri: str,\n",
        "    output_without_audio: str,\n",
        "    output_with_audio: str\n",
        "  ) -> list:\n",
        "  \"\"\"Selects videos for concatenation and returns them with an audio clip.\n",
        "\n",
        "  Args:\n",
        "    local_video_paths: A list of local paths to the video files.\n",
        "    transition: A VideoTransition object with the transition type and duration.\n",
        "    output_length: The desired length of the output video in seconds.\n",
        "    trim_location: Where to trim the videos (\"start\" or \"end\").\n",
        "    audio_inputs: A list of AudioInput objects for audio overlay.\n",
        "    gcs_uri: The GCS URI for uploading the final video.\n",
        "\n",
        "  Returns:\n",
        "    A list of selected videos\n",
        "    \"\"\"\n",
        "  try:\n",
        "    checkboxes = [\n",
        "        widgets.Checkbox(value=False, description=path)\n",
        "        for path in local_video_paths\n",
        "    ]\n",
        "    container = widgets.VBox(checkboxes)\n",
        "    display(container)\n",
        "\n",
        "    submit_button = widgets.Button(description=\"Submit\")\n",
        "    display(submit_button)\n",
        "\n",
        "    selected_videos = []\n",
        "\n",
        "    def on_button_clicked(button):\n",
        "      nonlocal selected_videos\n",
        "      clear_output(wait=True)\n",
        "      for path, checkbox in zip(local_video_paths, checkboxes):\n",
        "        if checkbox.value:\n",
        "          selected_videos.append(path)\n",
        "      final_video = concatenate_video_clips(\n",
        "          selected_videos,\n",
        "          transition,\n",
        "          output_length,\n",
        "          trim_location\n",
        "      )\n",
        "\n",
        "      gcs_uri_video = f'{gcs_uri}{output_without_audio}'\n",
        "      gcs_uri_audio = f'{gcs_uri}{output_with_audio}'\n",
        "\n",
        "      add_audio_clips_to_video(final_video, audio_inputs, gcs_uri_audio)\n",
        "      gcs.upload_file_to_gcs(final_video, gcs_uri_video)\n",
        "\n",
        "    submit_button.on_click(on_button_clicked)\n",
        "\n",
        "    return selected_videos\n",
        "  except Exception as e:\n",
        "    print(f'Error select_videos_for_concatenation {e}')\n",
        "\n",
        "\n",
        "\n",
        "def concatenate_video_clips(\n",
        "    videos: list[str],\n",
        "    transition: models.VideoTransition,\n",
        "    output_length: int,\n",
        "    trim_location: str\n",
        ") -> str:\n",
        "    \"\"\"Concatenates video clips with transitions and optional trimming.\n",
        "\n",
        "    Args:\n",
        "        videos: A list of paths to the video clips to concatenate.\n",
        "        transition: A VideoTransition object with transition type and duration.\n",
        "        output_length: The desired length of the output video in seconds.\n",
        "        trim_location: Where to trim the videos (\"start\" or \"end\").\n",
        "\n",
        "    Returns:\n",
        "        The path to the concatenated video file, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f'''Start concatenation of video files {videos}\n",
        "                 with transition {transition}...''')\n",
        "        if not videos:\n",
        "          raise ValueError('No video inputs provided.')\n",
        "\n",
        "        target_path = f'{TMP_STRING}/concat_target.mp4'\n",
        "        video_clips = []\n",
        "        target_resolution = set_target_resolution(videos[0])\n",
        "        for video in videos:\n",
        "          video_clips.append(\n",
        "              mp.VideoFileClip(\n",
        "                  video, target_resolution=target_resolution\n",
        "              )\n",
        "            )\n",
        "\n",
        "        video_clips = trim_clips(\n",
        "            video_clips,\n",
        "            transition.padding,\n",
        "            output_length,\n",
        "            trim_location\n",
        "        )\n",
        "        composed_clip = None\n",
        "        match transition.name:\n",
        "          case 'CROSS_FADE':\n",
        "            composed_clip = cross_fade(\n",
        "                video_clips,\n",
        "                transition.padding,\n",
        "                target_path\n",
        "            )\n",
        "          case 'FADE_IN':\n",
        "            composed_clip = fade_in(\n",
        "                video_clips,\n",
        "                transition.padding,\n",
        "                target_path\n",
        "            )\n",
        "          case 'SWIPE':\n",
        "            composed_clip = swipe(\n",
        "                video_clips,\n",
        "                transition.padding,\n",
        "                transition.side,\n",
        "                target_path\n",
        "            )\n",
        "          case 'SLIDE_IN':\n",
        "            composed_clip = slide_in(\n",
        "                video_clips,\n",
        "                transition.padding,\n",
        "                transition.side,\n",
        "                target_path\n",
        "            )\n",
        "          case _:\n",
        "            raise ValueError(f'Transition {transition.name} not supported.')\n",
        "\n",
        "        composed_clip.write_videofile(target_path, codec='libx264', logger=None)\n",
        "        composed_clip.close()\n",
        "\n",
        "    finally:\n",
        "        # Clean up temporary files\n",
        "        try:\n",
        "          for video in videos:\n",
        "            print(f'Removing temporary file {video}...')\n",
        "            os.remove(video)\n",
        "        except FileNotFoundError:\n",
        "          pass\n",
        "\n",
        "    print(f'Concatenation finished, video available: {target_path}')\n",
        "    return target_path\n",
        "\n",
        "def trim_clips(\n",
        "    video_clips: list[mp.VideoFileClip],\n",
        "    padding: float,\n",
        "    output_length: int,\n",
        "    trim_location: str\n",
        ") -> list[mp.VideoFileClip]:\n",
        "  \"\"\"Trims video clips to fit the desired output length.\n",
        "\n",
        "  Args:\n",
        "    video_clips: A list of moviepy VideoFileClip video clips.\n",
        "    padding: The duration of the padding (for transitions) in seconds.\n",
        "    output_length: The desired length of the output video in seconds.\n",
        "    trim_location: Where to trim the videos (\"start\" or \"end\").\n",
        "\n",
        "  Returns:\n",
        "    A list of trimmed moviepy VideoFileClip objects.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the trim_location is invalid or if output_length is <= 0.\n",
        "  \"\"\"\n",
        "  total_length = 0\n",
        "  for video in video_clips:\n",
        "    total_length += video.duration\n",
        "  total_length -= padding * (len(video_clips) - 1)\n",
        "\n",
        "  total_trim_length = total_length - output_length\n",
        "  print(f'Total length: {total_length} Output length: {output_length}')\n",
        "  if total_trim_length < 0 or not TRIM_ENABLED:\n",
        "    print('Trimming not required.')\n",
        "    return video_clips\n",
        "\n",
        "\n",
        "  number_of_intros_outros_clips = 2\n",
        "  number_of_veo_clips = len(video_clips) - number_of_intros_outros_clips\n",
        "  trim_length = total_trim_length / number_of_veo_clips\n",
        "  for index, video in enumerate(video_clips[1:-1]):\n",
        "    if trim_location == 'start':\n",
        "      start_time = -(video.duration - trim_length)\n",
        "      end_time = None\n",
        "    elif trim_location == 'end':\n",
        "      start_time = 0\n",
        "      end_time = video.duration - trim_length\n",
        "    else:\n",
        "      raise ValueError(f'Trim location {trim_location} not supported.')\n",
        "\n",
        "    trimmed_clip = video.subclipped(start_time, end_time)\n",
        "    video_clips[index + 1] = trimmed_clip\n",
        "\n",
        "  total_length = 0\n",
        "  for video in video_clips:\n",
        "    total_length += video.duration\n",
        "  total_length += padding * (len(video_clips) - 1)\n",
        "\n",
        "  print(f'Total length: {total_length} Output length: {output_length}')\n",
        "  return video_clips\n",
        "\n",
        "def calculate_audio_duration(\n",
        "    i: int, audio_inputs: list[models.AudioInput], video_duration: float\n",
        ") -> float:\n",
        "  \"\"\"Calculates the duration for each audio clip.\n",
        "  Args:\n",
        "    i: Index of the audio clip.\n",
        "    audio_inputs: List of AudioInput objects.\n",
        "    video_duration: Duration of the video in seconds.\n",
        "  Returns:\n",
        "    Duration of the audio clip in seconds.\n",
        "  \"\"\"\n",
        "  if not audio_inputs[i].duration:\n",
        "    next_start_time = (\n",
        "        audio_inputs[i + 1].start_time\n",
        "        if i < len(audio_inputs) - 1\n",
        "        else video_duration\n",
        "    )\n",
        "    duration = next_start_time - audio_inputs[i].start_time\n",
        "  else:\n",
        "    duration = audio_inputs[i].duration\n",
        "  return duration\n",
        "\n",
        "def load_audio_clips(\n",
        "    audio_inputs: list[models.AudioInput], video_duration: float\n",
        ") -> list[mp.AudioFileClip]:\n",
        "  \"\"\"Loads audio clips from a list of paths.\n",
        "  Args:\n",
        "    audio_inputs: List of paths to the audio files.\n",
        "    video_duration: Duration of the video in seconds.\n",
        "  Returns:\n",
        "    List of AudioFileClip objects.\n",
        "  \"\"\"\n",
        "  audio_clips = []\n",
        "  for i, audio_input in enumerate(audio_inputs):\n",
        "    check_file_exists(audio_input.path)\n",
        "    # If no duration calculate it based on the next start or video duration\n",
        "    duration = calculate_audio_duration(i, audio_inputs, video_duration)\n",
        "    audio_clip = (\n",
        "        mp.AudioFileClip(audio_input.path)\n",
        "        .with_start(audio_input.start_time)\n",
        "        .with_duration(duration)\n",
        "    )\n",
        "    audio_clips.append(audio_clip)\n",
        "  return audio_clips\n",
        "\n",
        "def add_audio_clips_to_video(\n",
        "    video_path: str,\n",
        "    audio_inputs: list[models.AudioInput],\n",
        "    gcs_uri: str\n",
        ") -> None:\n",
        "  \"\"\"Adds one or more audio clips to a video clip.\n",
        "  If several videos are provided without start times and duration, they will be\n",
        "  played in successive order, and the duration will be equal for each of the\n",
        "  audio clips.\n",
        "  Args:\n",
        "    video_path: Path to the video file.\n",
        "    audio_inputs: List of AudioInput objects, each representing an audio clip to\n",
        "      add.\n",
        "    gcs_uri: GCS URI of the video transition.\n",
        "  Raises:\n",
        "    ValueError: If the number of audio start times or durations does not match\n",
        "    the number of audio clips.\n",
        "  \"\"\"\n",
        "  check_file_exists(video_path)\n",
        "  target_path = '/tmp/concat_audio_target.mp4'\n",
        "  if not audio_inputs:\n",
        "    raise ValueError('No audio inputs provided.')\n",
        "  video = mp.VideoFileClip(video_path)\n",
        "  audio_clips = load_audio_clips(audio_inputs, video.duration)\n",
        "  final_audio = mp.CompositeAudioClip(audio_clips)\n",
        "  final_clip = video.with_audio(final_audio).with_duration(video.duration)\n",
        "  final_clip.write_videofile(\n",
        "      target_path,\n",
        "      codec='libx264',\n",
        "      audio_codec='aac',\n",
        "      logger=None\n",
        "  )\n",
        "\n",
        "  gcs.upload_file_to_gcs(target_path, gcs_uri)\n",
        "  video.close()\n",
        "  for audio in audio_clips:\n",
        "    audio.close()\n",
        "\n",
        "  final_clip.close()\n",
        "\n",
        "def set_target_resolution(video: str) -> tuple:\n",
        "  \"\"\"Sets the target resolution for a video based on its orientation.\n",
        "\n",
        "  Args:\n",
        "    video: The path to the video file.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the target width and height (in pixels).\n",
        "\n",
        "  \"\"\"\n",
        "  print('Retrieve the dimensions...')\n",
        "  clip = mp.VideoFileClip(video)\n",
        "  clip_dimension = clip.size\n",
        "  if clip_dimension[0] < clip_dimension[1]:\n",
        "    if RESIZED_IMAGE_WIDTH > RESIZED_IMAGE_HEIGHT:\n",
        "      dimension = (RESIZED_IMAGE_HEIGHT, RESIZED_IMAGE_WIDTH)\n",
        "    else:\n",
        "      dimension = (RESIZED_IMAGE_WIDTH, RESIZED_IMAGE_HEIGHT)\n",
        "  elif clip_dimension[0] > clip_dimension[1]:\n",
        "    if RESIZED_IMAGE_WIDTH > RESIZED_IMAGE_HEIGHT:\n",
        "      dimension = (RESIZED_IMAGE_WIDTH, RESIZED_IMAGE_HEIGHT)\n",
        "    else:\n",
        "      dimension = (RESIZED_IMAGE_HEIGHT, RESIZED_IMAGE_WIDTH)\n",
        "  else:\n",
        "    minimum_size = min(RESIZED_IMAGE_HEIGHT, RESIZED_IMAGE_WIDTH)\n",
        "    dimension = (minimum_size, minimum_size)\n",
        "  # print(f'Output dimensions: {dimension}')\n",
        "  return dimension\n",
        "\n",
        "def cross_fade(\n",
        "    video_clips: list[mp.VideoFileClip],\n",
        "    padding: float,\n",
        "    target_path: str,\n",
        "    side: str\n",
        "  ):\n",
        "  \"\"\"Applies a cross-fade transition between video clips and saves the result.\n",
        "\n",
        "  Args:\n",
        "    video_clips: A list of moviepy VideoFileClip objects to be concatenated.\n",
        "    padding: The duration of the cross-fade transition in seconds.\n",
        "    target_path: The path where the concatenated video will be saved.\n",
        "  \"\"\"\n",
        "  print('Concatenating video files...')\n",
        "  video_fx_list = []\n",
        "  composed_clip = video_clips[0]\n",
        "  opposite_side = get_opposite_side(side)\n",
        "  for index, video in enumerate(video_clips[1:]):\n",
        "      opposite_transition = mp.video.fx.CrossFadeOut(padding).copy()\n",
        "      transition = mp.video.fx.CrossFadeIn(padding).copy()\n",
        "\n",
        "      composed_effect = opposite_transition.apply(composed_clip)\n",
        "\n",
        "      video_effect = transition.apply(\n",
        "          video.with_start(composed_clip.duration - padding))\n",
        "      composed_clip = mp.CompositeVideoClip([composed_effect, video_effect])\n",
        "\n",
        "  return composed_clip\n",
        "\n",
        "def fade_in(\n",
        "    video_clips: list[mp.VideoFileClip],\n",
        "    padding: float,\n",
        "    target_path: str\n",
        "  ):\n",
        "  \"\"\"Applies a fade-in transition between video clips and saves the result.\n",
        "\n",
        "  Args:\n",
        "    video_clips: A list of moviepy VideoFileClip objects to be concatenated.\n",
        "    padding: The duration of the fade-in transition in seconds.\n",
        "    target_path: The path where the concatenated video will be saved.\n",
        "  \"\"\"\n",
        "\n",
        "  print('Concatenating video files...')\n",
        "  video_fx_list = [video_clips[0]]\n",
        "  idx = video_clips[0].duration - padding\n",
        "  for video in video_clips[1:]:\n",
        "      transition = mp.video.fx.CrossFadeIn(padding).copy()\n",
        "      video_fx_list.append(transition.apply(video.with_start(idx)))\n",
        "      idx += video.duration - padding\n",
        "\n",
        "  composed_clip = mp.CompositeVideoClip(video_fx_list)\n",
        "\n",
        "  return composed_clip\n",
        "\n",
        "def swipe(\n",
        "    video_clips: list[mp.VideoFileClip],\n",
        "    padding: float,\n",
        "    side: str,\n",
        "    target_path: str\n",
        "  ):\n",
        "  \"\"\"Applies a swipe transition between video clips and saves the result.\n",
        "\n",
        "  Args:\n",
        "    video_clips: A list of moviepy VideoFileClip objects to be concatenated.\n",
        "    padding: The duration of the transition (padding) in seconds.\n",
        "    side: The direction of the swipe ('left', 'right', 'top', 'bottom').\n",
        "    target_path: The path where the concatenated video will be saved.\n",
        "  \"\"\"\n",
        "  print('Concatenating video files...')\n",
        "  video_fx_list = []\n",
        "  composed_clip = video_clips[0]\n",
        "  opposite_side = get_opposite_side(side)\n",
        "  for index, video in enumerate(video_clips[1:]):\n",
        "      opposite_transition = mp.video.fx.SlideOut(padding, opposite_side).copy()\n",
        "      transition = mp.video.fx.SlideIn(padding, side).copy()\n",
        "\n",
        "      composed_effect = opposite_transition.apply(composed_clip)\n",
        "\n",
        "      video_effect = transition.apply(\n",
        "          video.with_start(composed_clip.duration - padding))\n",
        "      composed_clip = mp.CompositeVideoClip([composed_effect, video_effect])\n",
        "\n",
        "  return composed_clip\n",
        "\n",
        "def slide_in(\n",
        "    video_clips: list[mp.VideoFileClip],\n",
        "    padding: float,\n",
        "    side: str,\n",
        "    target_path: str\n",
        "  ):\n",
        "  \"\"\"Applies a slide-in transition to a video clip.\n",
        "  Args:\n",
        "    clip: The input video clip.\n",
        "    duration: The duration of the transition in seconds.\n",
        "    side: The direction from which the clip slides in\n",
        "     (\"left\", \"right\", \"top\", or \"bottom\"). Defaults to \"left\".\n",
        "\n",
        "  Returns:\n",
        "    The video clip with the slide-in transition applied.\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the side is invalid.\n",
        "  \"\"\"\n",
        "  print('Concatenating video files...')\n",
        "  video_fx_list = [video_clips[0]]\n",
        "  idx = video_clips[0].duration - padding\n",
        "  for index, video in enumerate(video_clips[1:]):\n",
        "      transition = mp.video.fx.SlideIn(padding, side).copy()\n",
        "      video_fx_list.append(transition.apply(video.with_start(idx)))\n",
        "      idx += video.duration - padding\n",
        "\n",
        "  composed_clip = mp.CompositeVideoClip(video_fx_list)\n",
        "\n",
        "  return composed_clip\n",
        "\n",
        "def get_opposite_side(side: str) -> str:\n",
        "  \"\"\"Returns the opposite side of a given direction.\n",
        "\n",
        "  Args:\n",
        "    side: The input side (\"left\", \"right\", \"top\", or \"bottom\").\n",
        "\n",
        "  Returns:\n",
        "    The opposite side (e.g. \"right\" for \"left\", \"left\" for \"right\"...).\n",
        "\n",
        "  Raises:\n",
        "    ValueError: If the input side is not one of the valid options.\n",
        "  \"\"\"\n",
        "  match side:\n",
        "    case 'left':\n",
        "      return 'right'\n",
        "    case 'right':\n",
        "      return 'left'\n",
        "    case 'top':\n",
        "      return 'bottom'\n",
        "    case 'bottom':\n",
        "      return 'top'\n",
        "    case _:\n",
        "      raise ValueError(f'Side {side} not supported.')\n",
        "\n",
        "def check_file_exists(file_path: str) -> None:\n",
        "  \"\"\"Checks if a file exists at the given path.\n",
        "  Args:\n",
        "    file_path: The path to the file.\n",
        "  Raises:\n",
        "    FileNotFoundError: If the file does not exist.\n",
        "  \"\"\"\n",
        "  if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f'Media file not found: {file_path}')\n",
        "\n",
        "def merge_arrays(intro_outro_videos, main_content_videos):\n",
        "    \"\"\"Merges intro/outro videos with main content videos.\n",
        "\n",
        "    This function takes two lists of videos: intro/outro videos and main\n",
        "    content videos. It inserts the first intro/outro video at the beginning,\n",
        "    appends all main content videos, and then adds the last intro/outro video\n",
        "    at the end, creating a merged video sequence. If intro_outro_videos is\n",
        "    empty, it returns main_content_videos directly.\n",
        "\n",
        "    Args:\n",
        "        intro_outro_videos: A list of intro/outro video paths.\n",
        "                            The first element is used as the intro, and the last\n",
        "                            as the outro. Can be empty. If intro_outro_videos is\n",
        "                            empty, it returns main_content_videos directly. If\n",
        "                            intro_outro_video contains only one video, it\n",
        "                            appends the same video to both begining and the end\n",
        "                            of main_content_videos.\n",
        "        main_content_videos: A list of main content video paths.\n",
        "\n",
        "    Returns:\n",
        "        A new list containing the merged video paths, or the\n",
        "        `main_content_videos` list if `intro_outro_videos` is empty.\n",
        "    \"\"\"\n",
        "    if len(intro_outro_videos) > 0:\n",
        "        return (\n",
        "            [intro_outro_videos[0]]\n",
        "            + main_content_videos\n",
        "            + [intro_outro_videos[-1]]\n",
        "        )\n",
        "    # If intro_outro_videos is empty, return main_content_videos directly\n",
        "    return main_content_videos\n",
        "\n",
        "def stitch_videos_with_transitions(\n",
        "    intro_outro_videos_uri: str,\n",
        "    veo_clips_uri: str,\n",
        "    audio_uri: str,\n",
        "    stiching_output_uri: str,\n",
        "    output_with_audio: str,\n",
        "    output_without_audio: str,\n",
        "    transition: str,\n",
        "    transition_duration: float,\n",
        "    transition_side: str,\n",
        "    desired_length: int,\n",
        "    trim_from: str,\n",
        ") -> None:\n",
        "    \"\"\"Stitches videos with transitions, overlays audio, and uploads to gcs.\n",
        "\n",
        "    Args:\n",
        "        intro_outro_videos_uri: The GCS URI of the folder with intros and outros.\n",
        "        veo_clips_uri: The GCS URI of the folder containing Veo overlay videos.\n",
        "        audio_uri: The GCS URI of the folder containing audio files.\n",
        "        stiching_output_uri: The GCS URI for the stitched video to be uploaded.\n",
        "        output_with_audio: The filename for the stitched video with audio.\n",
        "        output_without_audio: The filename for the stitched video without audio.\n",
        "        transition: The name of the transition to use (e.g., \"CROSS_FADE\"...).\n",
        "        transition_duration: The duration of the transition in seconds.\n",
        "        transition_side: The side of the transition (e.g., \"left\", \"right\").\n",
        "        desired_length: The desired length of the output video in seconds.\n",
        "        trim_from: Where to trim the video (\"start\" or \"end\").\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    # 1. Retrieve videos and audio from GCS\n",
        "    intro_outro_videos = gcs.retrieve_all_files_from_gcs_folder(\n",
        "        intro_outro_videos_uri\n",
        "    )\n",
        "    input_veo_clips = gcs.retrieve_all_files_from_gcs_folder(veo_clips_uri)\n",
        "    input_videos = merge_arrays(intro_outro_videos, input_veo_clips)\n",
        "    input_audio = gcs.retrieve_all_files_from_gcs_folder(audio_uri)\n",
        "\n",
        "    video_transition = models.VideoTransition(\n",
        "        name=transition,\n",
        "        padding=transition_duration,\n",
        "        side=transition_side\n",
        "    )\n",
        "\n",
        "    # 2. Download videos and audio locally\n",
        "    local_video_paths = gcs.download_files(input_videos)\n",
        "    local_audio_paths = [\n",
        "        models.AudioInput(path=path) for path in gcs.download_files(input_audio)\n",
        "    ]\n",
        "\n",
        "    # 3. Concatenate videos, apply transitions, and overlay audio\n",
        "    selected_videos = select_videos_for_concatenation(\n",
        "        local_video_paths,\n",
        "        video_transition,\n",
        "        desired_length,\n",
        "        trim_from,\n",
        "        local_audio_paths,\n",
        "        stiching_output_uri,\n",
        "        output_without_audio,\n",
        "        output_with_audio\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEBjgdAQq35g"
      },
      "outputs": [],
      "source": [
        "#@title Main functions\n",
        "def generate_and_select_videos(\n",
        "    images_uri: str,\n",
        "    output_uri_path: str,\n",
        "    resized_image_width: int,\n",
        "    resized_image_height: int,\n",
        "    original_background_color: models.RGBColor,\n",
        "    background_color: models.RGBColor\n",
        ") -> list[dict]:\n",
        "    \"\"\"Generates videos from images and returns user selected videos.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Resizes input images into landscape and portrait formats.\n",
        "    2. Recolors the background of the resized images.\n",
        "    3. Generates videos (VEOs) from the recolored images.\n",
        "    4. Presents the generated videos to the user for selection.\n",
        "    5. Returns a list of dictionaries containing the selected videos.\n",
        "\n",
        "    Args:\n",
        "        images_uri: The GCS URI of the folder containing the input images.\n",
        "        output_uri_path: The GCS URI of the folder where output will be stored.\n",
        "        resized_image_width: The desired width of the resized images.\n",
        "        resized_image_height: The desired height of the resized images.\n",
        "        original_background_color: The original background color of the images.\n",
        "        background_color: The desired background color of the images.\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries with information about a selected video.\n",
        "    \"\"\"\n",
        "    selected_products = utils.process_and_resize_images(\n",
        "        images_uri,\n",
        "        resized_image_width,\n",
        "        resized_image_height,\n",
        "        original_background_color,\n",
        "        output_uri_path\n",
        "    )\n",
        "    utils.recolor_background_and_upload(\n",
        "        selected_products,\n",
        "        output_uri_path,\n",
        "        original_background_color,\n",
        "        background_color\n",
        "    )\n",
        "    output_video_files = generate_videos_concurrently(selected_products)\n",
        "\n",
        "    selected_videos = []\n",
        "    get_user_choice_with_videos(output_video_files)\n",
        "    return selected_videos\n",
        "\n",
        "def process_and_stitch_videos(\n",
        "    input_videos: list[dict],\n",
        "    gcs_images: list[models.ImageInput],\n",
        "    text: models.TextInput,\n",
        "    image_video_path: str,\n",
        "    final_video_path: str,\n",
        "    intros_outros_uri: str,\n",
        "    veo_clips_uri: str,\n",
        "    audio_uri: str,\n",
        "    stitching_output_uri: str,\n",
        "    output_with_audio: str,\n",
        "    output_without_audio: str,\n",
        "    transition: models.VideoTransition,\n",
        "    output_length: int,\n",
        "    trim_from: str\n",
        ") -> None:\n",
        "    \"\"\"For selected videos, adds overlays and text, and stitches them together.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Processes selected videos by adding overlays and text.\n",
        "    2. Stitches the processed videos together with transitions.\n",
        "\n",
        "    Args:\n",
        "        selected_videos: A list of dictionaries with selected videos.\n",
        "        gcs_images: Images to be used for overlays.\n",
        "        text: Text to be added to the videos.\n",
        "        image_video_path: The path to save processed videos with image overlays.\n",
        "        final_video_path: The path to save the final overlayed video.\n",
        "        intros_outros_uri: The GCS URI for intro/outro videos.\n",
        "        veo_overlays_folder: The GCS URI for the folder containing VEO overlays.\n",
        "        audio_uri: The GCS URI for audio files.\n",
        "        stitching_output_uri: The GCS URI for the output of stitched videos.\n",
        "        output_with_audio: The filename for the output video with audio.\n",
        "        output_without_audio: The filename for the output video without audio.\n",
        "        transition:\n",
        "        output_length: The desired length of the output video.\n",
        "        trim_from: Where to trim the video to achieve the desired length.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    process_videos_with_overlays_and_text(\n",
        "        input_videos,\n",
        "        gcs_images,\n",
        "        text,\n",
        "        image_video_path,\n",
        "        final_video_path\n",
        "    )\n",
        "\n",
        "    stitch_videos_with_transitions(\n",
        "        intros_outros_uri,\n",
        "        veo_clips_uri,\n",
        "        audio_uri,\n",
        "        stitching_output_uri,\n",
        "        output_with_audio,\n",
        "        output_without_audio,\n",
        "        transition.name,\n",
        "        transition.padding,\n",
        "        transition.side,\n",
        "        output_length,\n",
        "        trim_from\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp3jvuXeLw2u"
      },
      "source": [
        "# Execute Main Functions\n",
        "\n",
        "## Part 1: Image resize, background recolor, veos generation and user selection\n",
        "\n",
        "The cell from part1 execute in approximately **1 minute**. However it waits on user input and is not finished until the user makes the selection and clicks the 'Submit' button.\n",
        "\n",
        "**Important**: If you run the next cell before you make the selection of the provided Veo generated videos and add the promo text, you will see errors. Please select the videos and promo texts before moving to the next one.\n",
        "\n",
        "## Part 2: Overlaying the Veos videos with image and text overlays and final videos stiching: intro, overlayed veos, outro and audio\n",
        "\n",
        "The cell from part2 executes in approximately **8 minutes**. However it waits on the user input to select which videos to stitch together: intro, overlayed veos and outro, and is not finished until the user makes the selection and clicks the 'Submit' button.\n",
        "\n",
        "**Important**: If you run the next cell before you make the selection of the provided video parts, you will see errors. Please select the video parts before moving to the next one.\n",
        "It will take in total between 15-20 minutes for this cell to finish."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HFX0Q3Wvy0b4"
      },
      "outputs": [],
      "source": [
        "#@title Part 1\n",
        "\n",
        "selected_videos = generate_and_select_videos(\n",
        "    IMAGES_URI,\n",
        "    OUTPUT_URI_PATH,\n",
        "    RESIZED_IMAGE_WIDTH,\n",
        "    RESIZED_IMAGE_HEIGHT,\n",
        "    ORIGINAL_BACKGROUND_COLOR,\n",
        "    BACKGROUND_COLOR\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ot-Utq148whF"
      },
      "outputs": [],
      "source": [
        "#@title Part 2\n",
        "# Please wait until previous cell is complete before running this one\n",
        "process_and_stitch_videos(\n",
        "    selected_videos,\n",
        "    GCS_IMAGES_TEST,\n",
        "    TEXT_TEST,\n",
        "    IMAGE_OVERLAYS_PATH,\n",
        "    FINAL_OVERLAYS_PATH,\n",
        "    INTROS_OUTROS_URI,\n",
        "    VEO_CLIPS_URI,\n",
        "    AUDIO_URI,\n",
        "    STITCHING_OUTPUT_URI,\n",
        "    OUTPUT_WITH_AUDIO,\n",
        "    OUTPUT_WITHOUT_AUDIO,\n",
        "    TRANSITION_TEST,\n",
        "    OUTPUT_LENGHT,\n",
        "    TRIM_FROM\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIKh039e-Kr3"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Upscale the local video to higher resolution\n",
        "def upscale_video(\n",
        "    input_video_path,\n",
        "    output_video_path,\n",
        "    target_width,\n",
        "    target_height):\n",
        "  \"\"\"Upscales a video to a higher resolution using ffmpeg.\n",
        "\n",
        "  Args:\n",
        "      input_video_path: The path to the input video file.\n",
        "      output_video_path: The path to the output video file.\n",
        "      target_width: The desired width of the upscaled video.\n",
        "      target_height: The desired height of the upscaled video.\n",
        "  \"\"\"\n",
        "  !ffmpeg -i {input_video_path} -vf scale={target_width}:{target_height} -c:a copy {output_video_path}\n",
        "\n",
        "# Example usage:\n",
        "output_video_path = f'{TMP_STRING}/upscaled_video.mp4'\n",
        "target_width = UPSCALE_FACTOR * RESIZED_IMAGE_WIDTH\n",
        "target_height = UPSCALE_FACTOR * RESIZED_IMAGE_WIDTH\n",
        "\n",
        "video_audio_filename = gcs.download_file_locally(\n",
        "    f'{STITCHING_OUTPUT_URI}{OUTPUT_WITH_AUDIO}')\n",
        "upscale_video(\n",
        "    video_audio_filename,\n",
        "    output_video_path,\n",
        "    target_width,\n",
        "    target_height\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XblYMBVdJfJ"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Clean up local files\n",
        "def cleanup_tmp_folder(folder_path):\n",
        "    \"\"\"\n",
        "    Removes all files and subdirectories within the specified folder.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder to clean up.\n",
        "    \"\"\"\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.exists(file_path):\n",
        "          os.remove(file_path)\n",
        "\n",
        "\n",
        "# Call the function with the TMP_STRING folder path\n",
        "cleanup_tmp_folder(TMP_STRING)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
